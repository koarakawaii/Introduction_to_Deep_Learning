{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lab\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys, cv2, time\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, init, autograd, nd, image\n",
    "from mxnet.gluon import data as gdata, utils as gutils, nn, loss as gloss\n",
    "from shutil import copyfile\n",
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = 'E:/Guan-Ming/Deep_ML/train_data_resize/'\n",
    "output_folder = 'E:/Guan-Ming/Deep_ML/train_data_final_transposed/'\n",
    "test_data_folder = 'E:/Guan-Ming/Deep_ML/test_data_resize/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'resized_two_50.jpg'\n",
    "# img = image.imread(target_folder+name)\n",
    "# color_aug = gdata.vision.transforms.RandomColorJitter(brightness=0.25, contrast=0.6)\n",
    "# for i in range(6):\n",
    "#     name_jitter = str('color_jitter%d_'%(i))\n",
    "#     img_color_jitter = color_aug(img)\n",
    "#     img_color_jitter_np = img_color_jitter.asnumpy()\n",
    "#     plt.imshow(img_color_jitter_np)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(target_folder)\n",
    "pictures_resized = []\n",
    "for f in files:\n",
    "    if '.jpg' in f:\n",
    "        pictures_resized.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(pictures_resized)):\n",
    "#for index in range(1):\n",
    "    \n",
    "    name = pictures_resized[index]\n",
    "    img = image.imread(target_folder+name)\n",
    "    img_np = img.asnumpy()\n",
    "    #num_jitter = 5\n",
    "    num_jitter = 3\n",
    "    #color_aug = gdata.vision.transforms.RandomColorJitter(brightness=0.25, contrast=0.6)\n",
    "    color_aug = gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.3)\n",
    "    aug_1 = gdata.vision.transforms.RandomFlipLeftRight()\n",
    "    aug_2 = gdata.vision.transforms.RandomFlipTopBottom()\n",
    "\n",
    "    success = False\n",
    "    while not success:\n",
    "        img_lr_flip = aug_1(img)\n",
    "        img_lr_flip = img_lr_flip.asnumpy()\n",
    "        if (img_lr_flip!=img_np).sum()!=0:\n",
    "            success = True\n",
    "            cv2.imwrite(output_folder+'left_right_flipped'+name, img_lr_flip[:,:,::-1])\n",
    "\n",
    "    success = False\n",
    "    while not success:\n",
    "        img_tb_flip = aug_2(img)\n",
    "        img_tb_flip = img_tb_flip.asnumpy()\n",
    "        if (img_tb_flip!=img_np).sum()!=0:\n",
    "            success = True\n",
    "            cv2.imwrite(output_folder+'top_down_flipped'+name, img_tb_flip[:,:,::-1])\n",
    "\n",
    "    for i in range(num_jitter):\n",
    "        name_jitter = str('color_jitter%d_'%(i))\n",
    "        img_color_jitter = color_aug(img)\n",
    "        img_color_jitter_np = img_color_jitter.asnumpy()\n",
    "        cv2.imwrite(output_folder+name_jitter+name, img_color_jitter_np[:,:,::-1])\n",
    "        success = False\n",
    "        while not success:\n",
    "            img_lr_flip = aug_1(img_color_jitter)\n",
    "            img_lr_flip = img_lr_flip.asnumpy()\n",
    "            if (img_lr_flip!=img_color_jitter_np).sum()!=0:\n",
    "                success = True\n",
    "                cv2.imwrite(output_folder+'left_right_flipped'+name_jitter+name, img_lr_flip[:,:,::-1])\n",
    "\n",
    "        success = False\n",
    "        while not success:\n",
    "            img_tb_flip = aug_2(img_color_jitter)\n",
    "            img_tb_flip = img_tb_flip.asnumpy()\n",
    "            if (img_tb_flip!=img_color_jitter_np).sum()!=0:\n",
    "                success = True\n",
    "                cv2.imwrite(output_folder+'top_down_flipped'+name_jitter+name, img_tb_flip[:,:,::-1])\n",
    "    copyfile(target_folder+name, output_folder+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(output_folder)\n",
    "pictures_list = []\n",
    "for f in files:\n",
    "    if '.jpg' in f:\n",
    "        pictures_list.append(f)\n",
    "        \n",
    "for name in pictures_list:\n",
    "    img = image.imread(output_folder+name)\n",
    "    img_np = img.asnumpy()\n",
    "    \n",
    "    img_transpose = np.zeros_like(img_np)\n",
    "    for channels in range(3):\n",
    "        img_transpose[:,:,channels] = img_np[:,:, channels].T\n",
    "    cv2.imwrite(output_folder+'transposed_'+name, img_transpose[:,:,::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum check pass!\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(output_folder)\n",
    "total_train_data = len(files)\n",
    "train_data_x = nd.zeros((total_train_data, 256, 256, 3)).astype(np.uint8)\n",
    "train_data_y = nd.zeros(total_train_data)\n",
    "total_num = 0\n",
    "\n",
    "for index, f in enumerate(files):\n",
    "    train_data_x[index] = image.imread(output_folder+f)\n",
    "    if 'zero' in f:\n",
    "        train_data_y[index] = 0\n",
    "        total_num += 1\n",
    "    elif 'one' in f:\n",
    "        train_data_y[index] = 1\n",
    "        total_num += 1\n",
    "    elif 'two' in f:\n",
    "        train_data_y[index] = 2\n",
    "        total_num += 1\n",
    "    elif 'three' in f:\n",
    "        train_data_y[index] = 3\n",
    "        total_num += 1\n",
    "    elif 'four' in f:\n",
    "        train_data_y[index] = 4\n",
    "        total_num += 1\n",
    "    elif 'five' in f:\n",
    "        train_data_y[index] = 5\n",
    "        total_num += 1\n",
    "    else:\n",
    "        raise RuntimeError('Cannot label training data!')\n",
    "\n",
    "if total_num == total_train_data:\n",
    "    print(\"Sum check pass!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6033889327634111, 0.5501308484707325, 0.5178991890040533],\n",
       " [0.2938110721034864, 0.2913514030355614, 0.2942829946844301])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_mean_r = (train_data_x[:,:,:,0].asnumpy()/255).mean()\n",
    "# train_mean_g = (train_data_x[:,:,:,1].asnumpy()/255).mean()\n",
    "# train_mean_b = (train_data_x[:,:,:,2].asnumpy()/255).mean()\n",
    "# train_std_r = (train_data_x[:,:,:,0].asnumpy()/255).std()\n",
    "# train_std_g = (train_data_x[:,:,:,1].asnumpy()/255).std()\n",
    "# train_std_b = (train_data_x[:,:,:,2].asnumpy()/255).std()\n",
    "# train_mean = [train_mean_r, train_mean_g, train_mean_b]\n",
    "# train_std = [train_std_r, train_std_g, train_std_b]\n",
    "train_mean = [0.6033889327634111, 0.5501308484707325, 0.5178991890040533]\n",
    "train_std = [0.2938110721034864, 0.2913514030355614, 0.2942829946844301]\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum check pass!\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(test_data_folder)\n",
    "total_test_data = len(files)\n",
    "test_data_x = nd.zeros((total_test_data, 256, 256, 3)).astype(np.uint8)\n",
    "test_data_y = nd.zeros(total_test_data)\n",
    "total_num = 0\n",
    "np.random.seed = 123456\n",
    "test_index_order = np.arange(total_test_data)\n",
    "np.random.shuffle(test_index_order)\n",
    "\n",
    "for index, index_shuffled in enumerate(test_index_order):\n",
    "    f = files[index_shuffled]\n",
    "    test_data_x[index] = image.imread(test_data_folder+f)\n",
    "    label = f.replace('resized_','').split('_', 1)[0]\n",
    "    if label == '0':\n",
    "        test_data_y[index] = 0\n",
    "        total_num += 1\n",
    "    elif label == '1':\n",
    "        test_data_y[index] = 1\n",
    "        total_num += 1\n",
    "    elif label == '2':\n",
    "        test_data_y[index] = 2\n",
    "        total_num += 1\n",
    "    elif label == '3':\n",
    "        test_data_y[index] = 3\n",
    "        total_num += 1\n",
    "    elif label == '4':\n",
    "        test_data_y[index] = 4\n",
    "        total_num += 1\n",
    "    elif label == '5':\n",
    "        test_data_y[index] = 5\n",
    "        total_num += 1\n",
    "    else:\n",
    "        raise RuntimeError('Cannot label testing data!')\n",
    "\n",
    "# for index, f in enumerate(files):\n",
    "#     test_data_x[index] = image.imread(test_data_folder+f)\n",
    "#     label = f.split('_', 1)[0]\n",
    "#     if label == '0':\n",
    "#         test_data_y[index] = 0\n",
    "#         total_num += 1\n",
    "#     elif label == '1':\n",
    "#         test_data_y[index] = 1\n",
    "#         total_num += 1\n",
    "#     elif label == '2':\n",
    "#         test_data_y[index] = 2\n",
    "#         total_num += 1\n",
    "#     elif label == '3':\n",
    "#         test_data_y[index] = 3\n",
    "#         total_num += 1\n",
    "#     elif label == '4':\n",
    "#         test_data_y[index] = 4\n",
    "#         total_num += 1\n",
    "#     elif label == '5':\n",
    "#         test_data_y[index] = 5\n",
    "#         total_num += 1\n",
    "#     else:\n",
    "#         raise RuntimeError('Cannot label training data!')\n",
    "        \n",
    "if total_num == total_test_data:\n",
    "    print(\"Sum check pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use gdata to form train dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = gdata.ArrayDataset(train_data_x, train_data_y)\n",
    "test_dataset = gdata.ArrayDataset(test_data_x, test_data_y)\n",
    "\n",
    "# index = np.random.randint(total_test_data)\n",
    "# count = 0\n",
    "# for X, y in test_dataset:\n",
    "#     if count==index:\n",
    "#         plt.imshow(X.asnumpy())\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "#         print('Label is %d .'%y)\n",
    "#         break\n",
    "#     count += 1\n",
    "    \n",
    "transformer = []\n",
    "transformer += [gdata.vision.transforms.ToTensor()] # transer the train data from shape (sample, H, W, channel) to (sample, channel, H, W) and rescale to between 0 and 1 \n",
    "transformer += [gdata.vision.transforms.Normalize(train_mean, train_std)]\n",
    "transformer = gdata.vision.transforms.Compose(transformer)\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "train_iter = gdata.DataLoader(train_dataset.transform_first(transformer), batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = gdata.DataLoader(test_dataset.transform_first(transformer), batch_size = batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = np.random.randint(batch_size)\n",
    "# count = 0\n",
    "# img = nd.zeros((256, 256, 3))\n",
    "# for X, y in test_iter:\n",
    "#     X = X*255\n",
    "#     img[:,:,0] = X[index,0,:,:]\n",
    "#     img[:,:,1] = X[index,1,:,:]\n",
    "#     img[:,:,2] = X[index,2,:,:]\n",
    "#     plt.imshow( ((img.asnumpy())//1).astype(np.uint8) )\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     print('Label is %d .'%y[index].asscalar())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "def train(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    average_time = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)            \n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        epoch_time = time.time() - start\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, epoch_time))\n",
    "        average_time += epoch_time\n",
    "    print('Average time per epoch is %.4f s.'%(average_time/num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Block):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs) \n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1, strides=strides) # this layer down-sampling the input\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)  # the layer doesn't down-sampling the input\n",
    "        \n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1, strides=strides)\n",
    "            # to match the dimension of feature map of conv1 + conv2, the strides must be set to be identicla to conv 1\n",
    "        else: \n",
    "            self.conv3 = None \n",
    "            \n",
    "        self.bn1 = nn.BatchNorm() \n",
    "        self.bn2 = nn.BatchNorm()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = nd.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y)) \n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "\n",
    "        return nd.relu(Y + X)\n",
    "    \n",
    "def resnet_block(num_channels, num_residuals, first_block=False): \n",
    "    blk = nn.Sequential() \n",
    "    for i in range(num_residuals): \n",
    "        if i == 0 and not first_block:\n",
    "            #net.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            #net.add(Residual(num_channels, strides=1))\n",
    "            blk.add(Residual(num_channels, strides=1))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_output_channels = [64, 128, 256, 512]\n",
    "num_residuals = [3, 4, 6, 3]\n",
    "\n",
    "net_RESNET = nn.Sequential() \n",
    "net_RESNET.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "        nn.BatchNorm(), \n",
    "        nn.Activation('relu'),\n",
    "        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "for i, num in enumerate(num_of_output_channels):\n",
    "    if i==0:\n",
    "        net_RESNET.add(resnet_block(num, num_residuals[i], first_block=True))\n",
    "    else:\n",
    "        net_RESNET.add(resnet_block(num, num_residuals[i]))\n",
    "    net_RESNET.add(nn.Dropout(0.3))\n",
    "\n",
    "net_RESNET.add(nn.GlobalAvgPool2D(), nn.Dense(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_RESNET.load_parameters('net_RESNET32_DROPOUT_train_data_transposed.params', ctx= ctx)\n",
    "net_RESNET.initialize(ctx=ctx, init=mx.init.Xavier(rnd_type ='gaussian', factor_type='in', magnitude=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input                           (64, 3, 256, 256)               0\n",
      "            Conv2D-1                          (64, 64, 128, 128)            9472\n",
      "         BatchNorm-2                          (64, 64, 128, 128)             256\n",
      "        Activation-3                          (64, 64, 128, 128)               0\n",
      "         MaxPool2D-4                            (64, 64, 64, 64)               0\n",
      "            Conv2D-5                            (64, 64, 64, 64)           36928\n",
      "         BatchNorm-6                            (64, 64, 64, 64)             256\n",
      "            Conv2D-7                            (64, 64, 64, 64)           36928\n",
      "         BatchNorm-8                            (64, 64, 64, 64)             256\n",
      "          Residual-9                            (64, 64, 64, 64)               0\n",
      "           Conv2D-10                            (64, 64, 64, 64)           36928\n",
      "        BatchNorm-11                            (64, 64, 64, 64)             256\n",
      "           Conv2D-12                            (64, 64, 64, 64)           36928\n",
      "        BatchNorm-13                            (64, 64, 64, 64)             256\n",
      "         Residual-14                            (64, 64, 64, 64)               0\n",
      "           Conv2D-15                            (64, 64, 64, 64)           36928\n",
      "        BatchNorm-16                            (64, 64, 64, 64)             256\n",
      "           Conv2D-17                            (64, 64, 64, 64)           36928\n",
      "        BatchNorm-18                            (64, 64, 64, 64)             256\n",
      "         Residual-19                            (64, 64, 64, 64)               0\n",
      "          Dropout-20                            (64, 64, 64, 64)               0\n",
      "           Conv2D-21                           (64, 128, 32, 32)           73856\n",
      "        BatchNorm-22                           (64, 128, 32, 32)             512\n",
      "           Conv2D-23                           (64, 128, 32, 32)          147584\n",
      "        BatchNorm-24                           (64, 128, 32, 32)             512\n",
      "           Conv2D-25                           (64, 128, 32, 32)            8320\n",
      "         Residual-26                           (64, 128, 32, 32)               0\n",
      "           Conv2D-27                           (64, 128, 32, 32)          147584\n",
      "        BatchNorm-28                           (64, 128, 32, 32)             512\n",
      "           Conv2D-29                           (64, 128, 32, 32)          147584\n",
      "        BatchNorm-30                           (64, 128, 32, 32)             512\n",
      "         Residual-31                           (64, 128, 32, 32)               0\n",
      "           Conv2D-32                           (64, 128, 32, 32)          147584\n",
      "        BatchNorm-33                           (64, 128, 32, 32)             512\n",
      "           Conv2D-34                           (64, 128, 32, 32)          147584\n",
      "        BatchNorm-35                           (64, 128, 32, 32)             512\n",
      "         Residual-36                           (64, 128, 32, 32)               0\n",
      "           Conv2D-37                           (64, 128, 32, 32)          147584\n",
      "        BatchNorm-38                           (64, 128, 32, 32)             512\n",
      "           Conv2D-39                           (64, 128, 32, 32)          147584\n",
      "        BatchNorm-40                           (64, 128, 32, 32)             512\n",
      "         Residual-41                           (64, 128, 32, 32)               0\n",
      "          Dropout-42                           (64, 128, 32, 32)               0\n",
      "           Conv2D-43                           (64, 256, 16, 16)          295168\n",
      "        BatchNorm-44                           (64, 256, 16, 16)            1024\n",
      "           Conv2D-45                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-46                           (64, 256, 16, 16)            1024\n",
      "           Conv2D-47                           (64, 256, 16, 16)           33024\n",
      "         Residual-48                           (64, 256, 16, 16)               0\n",
      "           Conv2D-49                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-50                           (64, 256, 16, 16)            1024\n",
      "           Conv2D-51                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-52                           (64, 256, 16, 16)            1024\n",
      "         Residual-53                           (64, 256, 16, 16)               0\n",
      "           Conv2D-54                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-55                           (64, 256, 16, 16)            1024\n",
      "           Conv2D-56                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-57                           (64, 256, 16, 16)            1024\n",
      "         Residual-58                           (64, 256, 16, 16)               0\n",
      "           Conv2D-59                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-60                           (64, 256, 16, 16)            1024\n",
      "           Conv2D-61                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-62                           (64, 256, 16, 16)            1024\n",
      "         Residual-63                           (64, 256, 16, 16)               0\n",
      "           Conv2D-64                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-65                           (64, 256, 16, 16)            1024\n",
      "           Conv2D-66                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-67                           (64, 256, 16, 16)            1024\n",
      "         Residual-68                           (64, 256, 16, 16)               0\n",
      "           Conv2D-69                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-70                           (64, 256, 16, 16)            1024\n",
      "           Conv2D-71                           (64, 256, 16, 16)          590080\n",
      "        BatchNorm-72                           (64, 256, 16, 16)            1024\n",
      "         Residual-73                           (64, 256, 16, 16)               0\n",
      "          Dropout-74                           (64, 256, 16, 16)               0\n",
      "           Conv2D-75                             (64, 512, 8, 8)         1180160\n",
      "        BatchNorm-76                             (64, 512, 8, 8)            2048\n",
      "           Conv2D-77                             (64, 512, 8, 8)         2359808\n",
      "        BatchNorm-78                             (64, 512, 8, 8)            2048\n",
      "           Conv2D-79                             (64, 512, 8, 8)          131584\n",
      "         Residual-80                             (64, 512, 8, 8)               0\n",
      "           Conv2D-81                             (64, 512, 8, 8)         2359808\n",
      "        BatchNorm-82                             (64, 512, 8, 8)            2048\n",
      "           Conv2D-83                             (64, 512, 8, 8)         2359808\n",
      "        BatchNorm-84                             (64, 512, 8, 8)            2048\n",
      "         Residual-85                             (64, 512, 8, 8)               0\n",
      "           Conv2D-86                             (64, 512, 8, 8)         2359808\n",
      "        BatchNorm-87                             (64, 512, 8, 8)            2048\n",
      "           Conv2D-88                             (64, 512, 8, 8)         2359808\n",
      "        BatchNorm-89                             (64, 512, 8, 8)            2048\n",
      "         Residual-90                             (64, 512, 8, 8)               0\n",
      "          Dropout-91                             (64, 512, 8, 8)               0\n",
      "  GlobalAvgPool2D-92                             (64, 512, 1, 1)               0\n",
      "            Dense-93                                     (64, 6)            3078\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 21309702\n",
      "   Trainable params: 21294470\n",
      "   Non-trainable params: 15232\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 21309702\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for X,_ in train_iter:\n",
    "    net_RESNET.summary(X.as_in_context(ctx))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 0.0300, train acc 0.990, test acc 0.690, time 82.3 sec\n",
      "epoch 2, loss 0.0289, train acc 0.990, test acc 0.637, time 83.3 sec\n",
      "epoch 3, loss 0.0279, train acc 0.992, test acc 0.687, time 82.6 sec\n",
      "epoch 4, loss 0.0266, train acc 0.992, test acc 0.663, time 82.6 sec\n",
      "epoch 5, loss 0.0223, train acc 0.992, test acc 0.720, time 83.3 sec\n",
      "epoch 6, loss 0.0241, train acc 0.992, test acc 0.667, time 83.1 sec\n",
      "epoch 7, loss 0.0185, train acc 0.994, test acc 0.707, time 82.5 sec\n",
      "epoch 8, loss 0.0142, train acc 0.995, test acc 0.680, time 82.7 sec\n",
      "epoch 9, loss 0.0159, train acc 0.995, test acc 0.677, time 83.4 sec\n",
      "epoch 10, loss 0.0199, train acc 0.994, test acc 0.727, time 83.1 sec\n",
      "epoch 11, loss 0.0155, train acc 0.995, test acc 0.647, time 83.2 sec\n",
      "epoch 12, loss 0.0219, train acc 0.994, test acc 0.677, time 82.5 sec\n",
      "epoch 13, loss 0.0158, train acc 0.995, test acc 0.600, time 82.8 sec\n",
      "epoch 14, loss 0.0143, train acc 0.995, test acc 0.700, time 82.3 sec\n",
      "epoch 15, loss 0.0134, train acc 0.995, test acc 0.697, time 82.5 sec\n",
      "epoch 16, loss 0.0113, train acc 0.996, test acc 0.700, time 82.7 sec\n",
      "epoch 17, loss 0.0142, train acc 0.995, test acc 0.707, time 82.2 sec\n",
      "epoch 18, loss 0.0127, train acc 0.996, test acc 0.673, time 82.7 sec\n",
      "epoch 19, loss 0.0130, train acc 0.996, test acc 0.720, time 82.8 sec\n",
      "epoch 20, loss 0.0127, train acc 0.996, test acc 0.680, time 82.3 sec\n",
      "Average time per epoch is 82.7401 s.\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = mx.optimizer.Adam(learning_rate=1e-4, beta1=0.9, beta2=0.9)\n",
    "trainer = gluon.Trainer(net_RESNET.collect_params(), optimizer=adam_optimizer)\n",
    "train(net_RESNET, train_iter, test_iter, batch_size, trainer, ctx, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model is 68.0000 % .\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of model is %.4f %% .'%(100*evaluate_accuracy(test_iter, net_RESNET, ctx)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_RESNET.save_parameters('C:/Users/lab/Jupyter_Notebook/Guan-Ming/Midterm_Project/NN_params/train_test_independent/RESNET18_DROPOUT_train_data_transposed.params')\n",
    "#===================================================================================\n",
    "# num_jitter = 3\n",
    "# color_aug = gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.3)\n",
    "#===================================================================================\n",
    "# num_of_output_channels = [64, 128, 256, 512]\n",
    "# num_residuals = 2\n",
    "\n",
    "# net_RESNET = nn.Sequential() \n",
    "# net_RESNET.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "#         nn.BatchNorm(), \n",
    "#         nn.Activation('relu'),\n",
    "#         nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "# for i, num in enumerate(num_of_output_channels):\n",
    "#     if i==0:\n",
    "#         net_RESNET.add(resnet_block(num, num_residuals, first_block=True))\n",
    "#     else:\n",
    "#         net_RESNET.add(resnet_block(num, num_residuals))\n",
    "#     net_RESNET.add(nn.Dropout(0.1))\n",
    "\n",
    "# net_RESNET.add(nn.GlobalAvgPool2D(), nn.Dense(6))\n",
    "#==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_RESNET.save_parameters('C:/Users/lab/Jupyter_Notebook/Guan-Ming/Midterm_Project/NN_params/train_test_independent/RESNET34_DROPOUT_train_data_transposed.params')\n",
    "#===================================================================================\n",
    "# num_jitter = 3\n",
    "# color_aug = gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.3)\n",
    "#===================================================================================\n",
    "# num_of_output_channels = [64, 128, 256, 512]\n",
    "# num_residuals = [3, 4, 6, 3]\n",
    "\n",
    "# net_RESNET = nn.Sequential() \n",
    "# net_RESNET.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "#         nn.BatchNorm(), \n",
    "#         nn.Activation('relu'),\n",
    "#         nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "# for i, num in enumerate(num_of_output_channels):\n",
    "#     if i==0:\n",
    "#         net_RESNET.add(resnet_block(num, num_residuals[i], first_block=True))\n",
    "#     else:\n",
    "#         net_RESNET.add(resnet_block(num, num_residuals[i]))\n",
    "#     net_RESNET.add(nn.Dropout(0.3))\n",
    "\n",
    "# net_RESNET.add(nn.GlobalAvgPool2D(), nn.Dense(6))\n",
    "#==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Block): \n",
    "    # c1 - c4 are the number of output channels for each layer in the path\n",
    "    def __init__(self, c1, c2, c3, c4, strides, **kwargs): \n",
    "        super(Inception, self).__init__(**kwargs) \n",
    "        # Path 1 is a single 1 x 1 convolutional layer \n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu') \n",
    "        # Path 2 is a 1 x 1 convolutional layer followed by a 3 x 3 \n",
    "        # convolutional layer \n",
    "        #self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        #self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1, activation='relu')\n",
    "        self.p2_1 = nn.Conv2D(c2, kernel_size=3, padding=1, activation='relu')\n",
    "        # Path 3 is a 1 x 1 convolutional layer followed by a 5 x 5\n",
    "        # convolutional layer \n",
    "        #self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu') \n",
    "        #self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2, activation='relu')\n",
    "        self.p3_1 = nn.Conv2D(c3, kernel_size=5, padding=2, activation='relu')\n",
    "        # Path 4 is a 3 x 3 maximum pooling layer followed by a 1 x 1\n",
    "        # convolutional layer\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1) \n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "        \n",
    "        self.drop = nn.Dropout(0.15)\n",
    "        self.pool = nn.MaxPool2D(pool_size=3, strides=strides, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x) \n",
    "        #p2 = self.p2_1(self.p2_1(x))\n",
    "        #p3 = self.p3_1(self.p3_1(x))\n",
    "        p2 = self.p2_1(x)\n",
    "        p3 = self.p3_1(x)\n",
    "        p4 = self.p4_2(self.p4_1(x)) \n",
    "        # Concatenate the outputs on the channel dimension \n",
    "        return self.drop(self.pool(nd.concat(p1, p2, p3, p4, dim=1)))\n",
    "\n",
    "class Residual_Inception(nn.Block):\n",
    "    def __init__(self, c1, c2, c3, c4, strides=1, **kwargs):\n",
    "        super(Residual_Inception, self).__init__(**kwargs) \n",
    "        self.incept = Inception(c1, c2, c3, c4, strides)\n",
    "        #num_channels = c1 + c2[1] + c3[1] + c4\n",
    "        num_channels = c1 + c2 + c3 + c4\n",
    "        \n",
    "        self.conv = nn.Conv2D(num_channels, kernel_size=1, strides=strides)\n",
    "        # to match the dimension of feature map of conv1 + conv2, the strides must be set to be identicla to conv 1\n",
    "        self.bn = nn.BatchNorm()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = self.bn(self.incept(X))\n",
    "        X = self.conv(X)\n",
    "\n",
    "        return nd.relu(X + Y)\n",
    "    \n",
    "def resnet_inception_block(num_channels, num_residuals, first_block=False): \n",
    "    blk = nn.Sequential() \n",
    "    for i in range(num_residuals): \n",
    "        if i == 0 and not first_block:\n",
    "            #net.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            blk.add(Residual_Inception(num_channels[0], num_channels[1], num_channels[2], num_channels[3], strides=2))\n",
    "        else:\n",
    "            #net.add(Residual(num_channels, strides=1))\n",
    "            blk.add(Residual_Inception(num_channels[0], num_channels[1], num_channels[2], num_channels[3], strides=1))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_of_output_channels = [(32, (32,64), (16, 32), 16), (32, (32,64), (16, 32), 16), (32, (32,64), (16, 32), 16)]\n",
    "num_of_output_channels = [(16, 32, 16, 8), (32, 64, 32, 8)]\n",
    "num_residuals = [2, 2]\n",
    "\n",
    "net_RESNET_INCEPT = nn.Sequential() \n",
    "net_RESNET_INCEPT.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
    "                      nn.BatchNorm(), \n",
    "                      nn.Activation('relu'),\n",
    "                      nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "for i, num in enumerate(num_of_output_channels):\n",
    "    if i==0:\n",
    "        net_RESNET_INCEPT.add(resnet_inception_block(num, num_residuals[i], first_block=True))\n",
    "    else:\n",
    "        net_RESNET_INCEPT.add(resnet_inception_block(num, num_residuals[i]))\n",
    "    #net_RESNET_INCEPT.add(nn.Dropout(0.3))\n",
    "\n",
    "net_RESNET_INCEPT.add(nn.GlobalAvgPool2D(), nn.Dense(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RESNET_INCEPT.initialize(ctx=ctx, init=mx.init.Xavier(rnd_type ='gaussian', factor_type='in', magnitude=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input                           (64, 3, 256, 256)               0\n",
      "            Conv2D-1                          (64, 64, 128, 128)            9472\n",
      "         BatchNorm-2                          (64, 64, 128, 128)             256\n",
      "        Activation-3                          (64, 64, 128, 128)               0\n",
      "         MaxPool2D-4                            (64, 64, 64, 64)               0\n",
      "        Activation-5                     <Symbol conv1_relu_fwd>               0\n",
      "        Activation-6                            (64, 32, 64, 64)               0\n",
      "            Conv2D-7                            (64, 32, 64, 64)            2080\n",
      "        Activation-8                     <Symbol conv2_relu_fwd>               0\n",
      "        Activation-9                            (64, 64, 64, 64)               0\n",
      "           Conv2D-10                            (64, 64, 64, 64)           36928\n",
      "       Activation-11                     <Symbol conv3_relu_fwd>               0\n",
      "       Activation-12                            (64, 32, 64, 64)               0\n",
      "           Conv2D-13                            (64, 32, 64, 64)           51232\n",
      "        MaxPool2D-14                            (64, 64, 64, 64)               0\n",
      "       Activation-15                     <Symbol conv4_relu_fwd>               0\n",
      "       Activation-16                            (64, 16, 64, 64)               0\n",
      "           Conv2D-17                            (64, 16, 64, 64)            1040\n",
      "        MaxPool2D-18                           (64, 144, 64, 64)               0\n",
      "          Dropout-19                           (64, 144, 64, 64)               0\n",
      "        Inception-20                           (64, 144, 64, 64)               0\n",
      "        BatchNorm-21                           (64, 144, 64, 64)             576\n",
      "           Conv2D-22                           (64, 144, 64, 64)            9360\n",
      "Residual_Inception-23                           (64, 144, 64, 64)               0\n",
      "       Activation-24                     <Symbol conv6_relu_fwd>               0\n",
      "       Activation-25                            (64, 32, 64, 64)               0\n",
      "           Conv2D-26                            (64, 32, 64, 64)            4640\n",
      "       Activation-27                     <Symbol conv7_relu_fwd>               0\n",
      "       Activation-28                            (64, 64, 64, 64)               0\n",
      "           Conv2D-29                            (64, 64, 64, 64)           83008\n",
      "       Activation-30                     <Symbol conv8_relu_fwd>               0\n",
      "       Activation-31                            (64, 32, 64, 64)               0\n",
      "           Conv2D-32                            (64, 32, 64, 64)          115232\n",
      "        MaxPool2D-33                           (64, 144, 64, 64)               0\n",
      "       Activation-34                     <Symbol conv9_relu_fwd>               0\n",
      "       Activation-35                            (64, 16, 64, 64)               0\n",
      "           Conv2D-36                            (64, 16, 64, 64)            2320\n",
      "        MaxPool2D-37                           (64, 144, 64, 64)               0\n",
      "          Dropout-38                           (64, 144, 64, 64)               0\n",
      "        Inception-39                           (64, 144, 64, 64)               0\n",
      "        BatchNorm-40                           (64, 144, 64, 64)             576\n",
      "           Conv2D-41                           (64, 144, 64, 64)           20880\n",
      "Residual_Inception-42                           (64, 144, 64, 64)               0\n",
      "  GlobalAvgPool2D-43                             (64, 144, 1, 1)               0\n",
      "            Dense-44                                     (64, 6)             870\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 338470\n",
      "   Trainable params: 337766\n",
      "   Non-trainable params: 704\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 338470\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for X,_ in train_iter:\n",
    "    net_RESNET_INCEPT.summary(X.as_in_context(ctx))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.6998, train acc 0.296, test acc 0.180, time 87.6 sec\n",
      "epoch 2, loss 1.4772, train acc 0.410, test acc 0.230, time 84.3 sec\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = mx.optimizer.Adam(learning_rate=1e-3, beta1=0.9, beta2=0.9)\n",
    "trainer = gluon.Trainer(net_RESNET_INCEPT.collect_params(), optimizer=adam_optimizer)\n",
    "train(net_RESNET_INCEPT, train_iter, test_iter, batch_size, trainer, ctx, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Block): \n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs) \n",
    "        self.net = nn.Sequential() \n",
    "        for _ in range(num_convs): \n",
    "            self.net.add(conv_block(num_channels))\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel \n",
    "            # dimension\n",
    "            X = nd.concat(X, Y, dim=1) \n",
    "\n",
    "        return X\n",
    "    \n",
    "def transition_block(num_channels):\n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1), \n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk\n",
    "\n",
    "def conv_block(num_channels): \n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_DENSENET = nn.Sequential()\n",
    "net_DENSENET.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), \n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('relu'), \n",
    "        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "num_channels, growth_rate = 64, 16\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4, 4]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    net_DENSENET.add(DenseBlock(num_convs, growth_rate))\n",
    "    net_DENSENET.add(nn.Dropout(0.3))\n",
    "#     # This is the number of output channels in the previous dense block \n",
    "    num_channels += num_convs * growth_rate\n",
    "    # A transition layer that haves the number of channels is added between\n",
    "    # the dense blocks \n",
    "    if i != len(num_convs_in_dense_blocks) - 1: \n",
    "        num_channels //= 2\n",
    "        net_DENSENET.add(transition_block(num_channels))\n",
    "                    \n",
    "net_DENSENET.add(nn.BatchNorm(), \n",
    "        nn.Activation('relu'), \n",
    "        nn.GlobalAvgPool2D(),\n",
    "        nn.Dense(6))\n",
    "\n",
    "net_DENSENET.initialize(ctx=ctx, init=mx.init.Xavier(rnd_type ='gaussian', factor_type='in', magnitude=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input                           (64, 3, 256, 256)               0\n",
      "            Conv2D-1                          (64, 64, 128, 128)            9472\n",
      "         BatchNorm-2                          (64, 64, 128, 128)             256\n",
      "        Activation-3                          (64, 64, 128, 128)               0\n",
      "         MaxPool2D-4                            (64, 64, 64, 64)               0\n",
      "         BatchNorm-5                            (64, 64, 64, 64)             256\n",
      "        Activation-6                            (64, 64, 64, 64)               0\n",
      "            Conv2D-7                            (64, 16, 64, 64)            9232\n",
      "         BatchNorm-8                            (64, 80, 64, 64)             320\n",
      "        Activation-9                            (64, 80, 64, 64)               0\n",
      "           Conv2D-10                            (64, 16, 64, 64)           11536\n",
      "        BatchNorm-11                            (64, 96, 64, 64)             384\n",
      "       Activation-12                            (64, 96, 64, 64)               0\n",
      "           Conv2D-13                            (64, 16, 64, 64)           13840\n",
      "        BatchNorm-14                           (64, 112, 64, 64)             448\n",
      "       Activation-15                           (64, 112, 64, 64)               0\n",
      "           Conv2D-16                            (64, 16, 64, 64)           16144\n",
      "       DenseBlock-17                           (64, 128, 64, 64)               0\n",
      "          Dropout-18                           (64, 128, 64, 64)               0\n",
      "        BatchNorm-19                           (64, 128, 64, 64)             512\n",
      "       Activation-20                           (64, 128, 64, 64)               0\n",
      "           Conv2D-21                            (64, 64, 64, 64)            8256\n",
      "        AvgPool2D-22                            (64, 64, 32, 32)               0\n",
      "        BatchNorm-23                            (64, 64, 32, 32)             256\n",
      "       Activation-24                            (64, 64, 32, 32)               0\n",
      "           Conv2D-25                            (64, 16, 32, 32)            9232\n",
      "        BatchNorm-26                            (64, 80, 32, 32)             320\n",
      "       Activation-27                            (64, 80, 32, 32)               0\n",
      "           Conv2D-28                            (64, 16, 32, 32)           11536\n",
      "        BatchNorm-29                            (64, 96, 32, 32)             384\n",
      "       Activation-30                            (64, 96, 32, 32)               0\n",
      "           Conv2D-31                            (64, 16, 32, 32)           13840\n",
      "        BatchNorm-32                           (64, 112, 32, 32)             448\n",
      "       Activation-33                           (64, 112, 32, 32)               0\n",
      "           Conv2D-34                            (64, 16, 32, 32)           16144\n",
      "       DenseBlock-35                           (64, 128, 32, 32)               0\n",
      "          Dropout-36                           (64, 128, 32, 32)               0\n",
      "        BatchNorm-37                           (64, 128, 32, 32)             512\n",
      "       Activation-38                           (64, 128, 32, 32)               0\n",
      "           Conv2D-39                            (64, 64, 32, 32)            8256\n",
      "        AvgPool2D-40                            (64, 64, 16, 16)               0\n",
      "        BatchNorm-41                            (64, 64, 16, 16)             256\n",
      "       Activation-42                            (64, 64, 16, 16)               0\n",
      "           Conv2D-43                            (64, 16, 16, 16)            9232\n",
      "        BatchNorm-44                            (64, 80, 16, 16)             320\n",
      "       Activation-45                            (64, 80, 16, 16)               0\n",
      "           Conv2D-46                            (64, 16, 16, 16)           11536\n",
      "        BatchNorm-47                            (64, 96, 16, 16)             384\n",
      "       Activation-48                            (64, 96, 16, 16)               0\n",
      "           Conv2D-49                            (64, 16, 16, 16)           13840\n",
      "        BatchNorm-50                           (64, 112, 16, 16)             448\n",
      "       Activation-51                           (64, 112, 16, 16)               0\n",
      "           Conv2D-52                            (64, 16, 16, 16)           16144\n",
      "       DenseBlock-53                           (64, 128, 16, 16)               0\n",
      "          Dropout-54                           (64, 128, 16, 16)               0\n",
      "        BatchNorm-55                           (64, 128, 16, 16)             512\n",
      "       Activation-56                           (64, 128, 16, 16)               0\n",
      "           Conv2D-57                            (64, 64, 16, 16)            8256\n",
      "        AvgPool2D-58                              (64, 64, 8, 8)               0\n",
      "        BatchNorm-59                              (64, 64, 8, 8)             256\n",
      "       Activation-60                              (64, 64, 8, 8)               0\n",
      "           Conv2D-61                              (64, 16, 8, 8)            9232\n",
      "        BatchNorm-62                              (64, 80, 8, 8)             320\n",
      "       Activation-63                              (64, 80, 8, 8)               0\n",
      "           Conv2D-64                              (64, 16, 8, 8)           11536\n",
      "        BatchNorm-65                              (64, 96, 8, 8)             384\n",
      "       Activation-66                              (64, 96, 8, 8)               0\n",
      "           Conv2D-67                              (64, 16, 8, 8)           13840\n",
      "        BatchNorm-68                             (64, 112, 8, 8)             448\n",
      "       Activation-69                             (64, 112, 8, 8)               0\n",
      "           Conv2D-70                              (64, 16, 8, 8)           16144\n",
      "       DenseBlock-71                             (64, 128, 8, 8)               0\n",
      "          Dropout-72                             (64, 128, 8, 8)               0\n",
      "        BatchNorm-73                             (64, 128, 8, 8)             512\n",
      "       Activation-74                             (64, 128, 8, 8)               0\n",
      "           Conv2D-75                              (64, 64, 8, 8)            8256\n",
      "        AvgPool2D-76                              (64, 64, 4, 4)               0\n",
      "        BatchNorm-77                              (64, 64, 4, 4)             256\n",
      "       Activation-78                              (64, 64, 4, 4)               0\n",
      "           Conv2D-79                              (64, 16, 4, 4)            9232\n",
      "        BatchNorm-80                              (64, 80, 4, 4)             320\n",
      "       Activation-81                              (64, 80, 4, 4)               0\n",
      "           Conv2D-82                              (64, 16, 4, 4)           11536\n",
      "        BatchNorm-83                              (64, 96, 4, 4)             384\n",
      "       Activation-84                              (64, 96, 4, 4)               0\n",
      "           Conv2D-85                              (64, 16, 4, 4)           13840\n",
      "        BatchNorm-86                             (64, 112, 4, 4)             448\n",
      "       Activation-87                             (64, 112, 4, 4)               0\n",
      "           Conv2D-88                              (64, 16, 4, 4)           16144\n",
      "       DenseBlock-89                             (64, 128, 4, 4)               0\n",
      "          Dropout-90                             (64, 128, 4, 4)               0\n",
      "        BatchNorm-91                             (64, 128, 4, 4)             512\n",
      "       Activation-92                             (64, 128, 4, 4)               0\n",
      "  GlobalAvgPool2D-93                             (64, 128, 1, 1)               0\n",
      "            Dense-94                                     (64, 6)             774\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 306886\n",
      "   Trainable params: 301958\n",
      "   Non-trainable params: 4928\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 306886\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for X,_ in train_iter:\n",
    "    net_DENSENET.summary(X.as_in_context(ctx))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.5889, train acc 0.334, test acc 0.183, time 69.5 sec\n",
      "epoch 2, loss 1.1107, train acc 0.552, test acc 0.333, time 69.2 sec\n",
      "epoch 3, loss 0.7759, train acc 0.697, test acc 0.317, time 69.7 sec\n",
      "epoch 4, loss 0.5791, train acc 0.781, test acc 0.423, time 69.3 sec\n",
      "epoch 5, loss 0.4588, train acc 0.829, test acc 0.483, time 69.3 sec\n",
      "epoch 6, loss 0.3636, train acc 0.869, test acc 0.383, time 69.4 sec\n",
      "epoch 7, loss 0.3104, train acc 0.888, test acc 0.487, time 69.3 sec\n",
      "epoch 8, loss 0.2864, train acc 0.892, test acc 0.570, time 69.2 sec\n",
      "epoch 9, loss 0.2281, train acc 0.916, test acc 0.510, time 69.4 sec\n",
      "epoch 10, loss 0.2055, train acc 0.926, test acc 0.543, time 69.6 sec\n",
      "epoch 11, loss 0.1709, train acc 0.939, test acc 0.553, time 69.5 sec\n",
      "epoch 12, loss 0.1439, train acc 0.947, test acc 0.547, time 69.4 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bef6a498c6d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0madam_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_DENSENET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madam_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_DENSENET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-a3c0bc063b67>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mtrain_l_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\gluon\\data\\dataloader.py\u001b[0m in \u001b[0;36msame_process_iter\u001b[1;34m()\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0msame_process_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_sampler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batchify_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_pinned\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_device_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\gluon\\data\\dataloader.py\u001b[0m in \u001b[0;36mdefault_batchify_fn\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_batchify_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\gluon\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_batchify_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\gluon\\data\\dataloader.py\u001b[0m in \u001b[0;36mdefault_batchify_fn\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\ndarray\\utils.py\u001b[0m in \u001b[0;36marray\u001b[1;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_sparse_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36marray\u001b[1;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[0;32m   2503\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source_array must be array like object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2504\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2505\u001b[1;33m     \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2506\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mindexing_dispatch_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_indexing_dispatch_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_NDARRAY_BASIC_INDEXING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_nd_basic_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_NDARRAY_ADVANCED_INDEXING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_nd_advanced_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36m_set_nd_basic_indexing\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sync_copyfrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# value might be a list or a tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mvalue_nd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_value_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36m_sync_copyfrom\u001b[1;34m(self, source_array)\u001b[0m\n\u001b[0;32m    879\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m             \u001b[0msource_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m             ctypes.c_size_t(source_array.size)))\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adam_optimizer = mx.optimizer.Adam(learning_rate=5e-4, beta1=0.9, beta2=0.9)\n",
    "trainer = gluon.Trainer(net_DENSENET.collect_params(), optimizer=adam_optimizer)\n",
    "train(net_DENSENET, train_iter, test_iter, batch_size, trainer, ctx, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model is 71.6667 % .\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of model is %.4f %% .'%(100*evaluate_accuracy(test_iter, net_DENSENET, ctx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_DENSENET.save_parameters('C:/Users/lab/Jupyter_Notebook/Guan-Ming/Midterm_Project/NN_params/train_test_independent/DENSENET_DROPOUT_train_data_transposed.params')\n",
    "#===================================================================================\n",
    "# num_jitter = 3\n",
    "# color_aug = gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.3)\n",
    "#===================================================================================\n",
    "# net_DENSENET = nn.Sequential()\n",
    "# net_DENSENET.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), \n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation('relu'), \n",
    "#         nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "# num_channels, growth_rate = 64, 16\n",
    "# num_convs_in_dense_blocks = [4, 4, 4, 4, 4]\n",
    "\n",
    "# for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "#     net_DENSENET.add(DenseBlock(num_convs, growth_rate))\n",
    "#     net_DENSENET.add(nn.Dropout(0.15))\n",
    "#     # This is the number of output channels in the previous dense block \n",
    "#     num_channels += num_convs * growth_rate\n",
    "#     # A transition layer that haves the number of channels is added between\n",
    "#     # the dense blocks \n",
    "#     if i != len(num_convs_in_dense_blocks) - 1: \n",
    "#         num_channels //= 2\n",
    "#         net_DENSENET.add(transition_block(num_channels))\n",
    "                    \n",
    "# net_DENSENET.add(nn.BatchNorm(), \n",
    "#         nn.Activation('relu'), \n",
    "#         nn.GlobalAvgPool2D(),\n",
    "#         nn.Dense(6))\n",
    "\n",
    "# net_DENSENET.initialize(ctx=ctx, init=mx.init.Xavier(rnd_type ='gaussian', factor_type='in', magnitude=2.0))\n",
    "#==================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = gdata.vision.FashionMNIST(root=os.path.join('~', '.mxnet', 'datasets', 'fashion-mnist'), train=True)\n",
    "mnist_test = gdata.vision.FashionMNIST(root=os.path.join('~', '.mxnet', 'datasets', 'fashion-mnist'), train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "for X, _ in mnist_train:\n",
    "    print(X.shape, X.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 3, 2, 8, 6, 5, 0])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test[:10][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
