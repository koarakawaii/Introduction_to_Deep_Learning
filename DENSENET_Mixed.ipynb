{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lab\\AppData\\Local\\Continuum\\anaconda3\\envs\\mxnet-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys, cv2, time\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, init, autograd, nd, image\n",
    "from mxnet.gluon import data as gdata, utils as gutils, nn, loss as gloss\n",
    "from shutil import copyfile\n",
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from ipywidgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_folder = 'E:/Guan-Ming/Deep_ML/all_data_resize/'\n",
    "# output_folder = 'E:/Guan-Ming/Deep_ML/all_data_final_transposed/'\n",
    "# test_data_folder = 'E:/Guan-Ming/Deep_ML/test_data_resize/'\n",
    "# extra_validation_target_folder = 'E:/Guan-Ming/Deep_ML/extra_validation_data_resize/'\n",
    "\n",
    "output_folder = 'Train_Data/'\n",
    "test_data_folder = 'Test_Data/'\n",
    "extra_validation_target_folder = 'Extra_Validation_Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum check pass!\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(output_folder)\n",
    "total_data = len(files)\n",
    "data_x = nd.zeros((total_data, 256, 256, 3)).astype(np.uint8)\n",
    "data_y = nd.zeros(total_data)\n",
    "total_num = 0\n",
    "\n",
    "for index, f in enumerate(files):\n",
    "    data_x[index] = image.imread(output_folder+f)\n",
    "    label = f.split('_',1)[0]\n",
    "    if label == '0':\n",
    "        data_y[index] = 0\n",
    "        total_num += 1\n",
    "    elif label == '1':\n",
    "        data_y[index] = 1\n",
    "        total_num += 1\n",
    "    elif label == '2':\n",
    "        data_y[index] = 2\n",
    "        total_num += 1\n",
    "    elif label == '3':\n",
    "        data_y[index] = 3\n",
    "        total_num += 1\n",
    "    elif label == '4':\n",
    "        data_y[index] = 4\n",
    "        total_num += 1\n",
    "    elif label == '5':\n",
    "        data_y[index] = 5\n",
    "        total_num += 1\n",
    "    else:\n",
    "        raise RuntimeError('Cannot label training data!')\n",
    "\n",
    "if total_num == total_data:\n",
    "    print(\"Sum check pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum check pass!\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(test_data_folder)\n",
    "total_test_data = len(files)\n",
    "test_data_x = nd.zeros((total_test_data, 256, 256, 3)).astype(np.uint8)\n",
    "test_data_y = nd.zeros(total_test_data)\n",
    "total_num = 0\n",
    "\n",
    "for index in range(total_test_data):\n",
    "    f = files[index]\n",
    "    test_data_x[index] = image.imread(test_data_folder+f)\n",
    "    label = f.split('_', 1)[0]\n",
    "    if label == '0':\n",
    "        test_data_y[index] = 0\n",
    "        total_num += 1\n",
    "    elif label == '1':\n",
    "        test_data_y[index] = 1\n",
    "        total_num += 1\n",
    "    elif label == '2':\n",
    "        test_data_y[index] = 2\n",
    "        total_num += 1\n",
    "    elif label == '3':\n",
    "        test_data_y[index] = 3\n",
    "        total_num += 1\n",
    "    elif label == '4':\n",
    "        test_data_y[index] = 4\n",
    "        total_num += 1\n",
    "    elif label == '5':\n",
    "        test_data_y[index] = 5\n",
    "        total_num += 1\n",
    "    else:\n",
    "        raise RuntimeError('Cannot label testation data!')\n",
    "        \n",
    "if total_num == total_test_data:\n",
    "    print(\"Sum check pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Extra Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum check pass!\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(extra_validation_target_folder)\n",
    "total_extra_validation_data = len(files)\n",
    "extra_validation_data_x = nd.zeros((total_extra_validation_data, 256, 256, 3)).astype(np.uint8)\n",
    "extra_validation_data_y = nd.zeros(total_extra_validation_data)\n",
    "total_num = 0\n",
    "\n",
    "for index, f in enumerate(files):\n",
    "    extra_validation_data_x[index] = image.imread(extra_validation_target_folder+f)\n",
    "\n",
    "    label = f.split('_',1)[0]\n",
    "    if label =='0':\n",
    "        extra_validation_data_y[index] = 0\n",
    "        total_num += 1\n",
    "    elif label =='1':\n",
    "        extra_validation_data_y[index] = 1\n",
    "        total_num += 1\n",
    "    elif label =='2':\n",
    "        extra_validation_data_y[index] = 2\n",
    "        total_num += 1\n",
    "    elif label =='3':\n",
    "        extra_validation_data_y[index] = 3\n",
    "        total_num += 1\n",
    "    elif label =='4':\n",
    "        extra_validation_data_y[index] = 4\n",
    "        total_num += 1\n",
    "    elif label =='5':\n",
    "        extra_validation_data_y[index] = 5\n",
    "        total_num += 1\n",
    "    else:\n",
    "        raise RuntimeError('Cannot label training data!')\n",
    "#    break\n",
    "\n",
    "if total_num == total_extra_validation_data:\n",
    "    print(\"Sum check pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Valid Spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = np.arange(total_data)\n",
    "train_size = 0.65\n",
    "np.random.seed = 61387454\n",
    "np.random.shuffle(data_index)\n",
    "train_index = data_index[:np.int32(total_data*train_size)]\n",
    "valid_index = data_index[np.int32(total_data*train_size):]\n",
    "train_data_x = data_x[train_index]\n",
    "train_data_y = data_y[train_index]\n",
    "valid_data_x = data_x[valid_index]\n",
    "valid_data_y = data_y[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = [0.592143940485608, 0.5347022953513785, 0.4981124834847486]\n",
    "train_std = [0.2880660398985942, 0.2820819880842514, 0.282954693408029]\n",
    "extra_validation_mean = [0.48470740652217936, 0.4379602084974611, 0.37406493007969804]\n",
    "extra_validation_std = [0.2525134266173694, 0.23321502528915997, 0.2119784063910089]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = gdata.ArrayDataset(train_data_x, train_data_y)\n",
    "valid_dataset = gdata.ArrayDataset(valid_data_x, valid_data_y)\n",
    "test_dataset = gdata.ArrayDataset(test_data_x, test_data_y)\n",
    "extra_validation_dataset = gdata.ArrayDataset(extra_validation_data_x, extra_validation_data_y)\n",
    "    \n",
    "transformer = []\n",
    "transformer += [gdata.vision.transforms.ToTensor()] # transer the train data from shape (sample, H, W, channel) to (sample, channel, H, W) and rescale to between 0 and 1 \n",
    "transformer += [gdata.vision.transforms.Normalize(train_mean, train_std)]\n",
    "transformer = gdata.vision.transforms.Compose(transformer)\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "train_iter = gdata.DataLoader(train_dataset.transform_first(transformer), batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
    "valid_iter = gdata.DataLoader(valid_dataset.transform_first(transformer), batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_iter = gdata.DataLoader(test_dataset.transform_first(transformer), batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "extra_transformer = []\n",
    "extra_transformer += [gdata.vision.transforms.ToTensor()] # transer the train data from shape (sample, H, W, channel) to (sample, channel, H, W) and rescale to between 0 and 1 \n",
    "extra_transformer += [gdata.vision.transforms.Normalize(extra_validation_mean, extra_validation_std)]\n",
    "extra_transformer = gdata.vision.transforms.Compose(extra_transformer)\n",
    "extra_validation_iter = gdata.DataLoader(extra_validation_dataset.transform_first(extra_transformer), batch_size = batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "def train(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    average_time = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)            \n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        epoch_time = time.time() - start\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc, epoch_time))\n",
    "        average_time += epoch_time\n",
    "    print('Average time per epoch is %.4f s.'%(average_time/num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = try_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DENSENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Block): \n",
    "    def __init__(self, num_convs, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs) \n",
    "        self.net = nn.Sequential() \n",
    "        for _ in range(num_convs): \n",
    "            self.net.add(conv_block(num_channels))\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel \n",
    "            # dimension\n",
    "            X = nd.concat(X, Y, dim=1) \n",
    "\n",
    "        return X\n",
    "    \n",
    "def transition_block(num_channels):\n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1), \n",
    "            nn.AvgPool2D(pool_size=2, strides=2))\n",
    "    return blk\n",
    "\n",
    "def conv_block(num_channels): \n",
    "    blk = nn.Sequential() \n",
    "    blk.add(nn.BatchNorm(), \n",
    "            nn.Activation('relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=3, padding=1))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_DENSENET = nn.Sequential()\n",
    "net_DENSENET.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), \n",
    "        nn.BatchNorm(),\n",
    "        nn.Activation('relu'), \n",
    "        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "num_channels, growth_rate = 32, 20\n",
    "num_convs_in_dense_blocks = [4, 8, 16, 32]\n",
    "\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    net_DENSENET.add(DenseBlock(num_convs, growth_rate))\n",
    "    net_DENSENET.add(nn.Dropout(0.15))\n",
    "#     # This is the number of output channels in the previous dense block \n",
    "    num_channels += num_convs * growth_rate\n",
    "    # A transition layer that haves the number of channels is added between\n",
    "    # the dense blocks \n",
    "    if i != len(num_convs_in_dense_blocks) - 1: \n",
    "        num_channels //= 2\n",
    "        net_DENSENET.add(transition_block(num_channels))\n",
    "                    \n",
    "net_DENSENET.add(nn.BatchNorm(), \n",
    "        nn.Activation('relu'), \n",
    "        nn.GlobalAvgPool2D(),\n",
    "        nn.Dense(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize or Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_DENSENET.load_parameters('Weight_Train_Test_Mixed/DENSENET_DROPOUT_train_data_transposed_2.params', ctx= ctx)\n",
    "#net_DENSENET.load_parameters('C:/Users/lab/Jupyter_Notebook/Guan-Ming/Midterm_Project/NN_params/train_test_mixed/DENSENET_DROPOUT_train_data_transposed_2.params', ctx = ctx)\n",
    "#net_DENSENET.initialize(ctx=ctx, init=mx.init.Xavier(rnd_type ='gaussian', factor_type='in', magnitude=2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input                           (64, 3, 256, 256)               0\n",
      "            Conv2D-1                          (64, 64, 128, 128)            9472\n",
      "         BatchNorm-2                          (64, 64, 128, 128)             256\n",
      "        Activation-3                          (64, 64, 128, 128)               0\n",
      "         MaxPool2D-4                            (64, 64, 64, 64)               0\n",
      "         BatchNorm-5                            (64, 64, 64, 64)             256\n",
      "        Activation-6                            (64, 64, 64, 64)               0\n",
      "            Conv2D-7                            (64, 20, 64, 64)           11540\n",
      "         BatchNorm-8                            (64, 84, 64, 64)             336\n",
      "        Activation-9                            (64, 84, 64, 64)               0\n",
      "           Conv2D-10                            (64, 20, 64, 64)           15140\n",
      "        BatchNorm-11                           (64, 104, 64, 64)             416\n",
      "       Activation-12                           (64, 104, 64, 64)               0\n",
      "           Conv2D-13                            (64, 20, 64, 64)           18740\n",
      "        BatchNorm-14                           (64, 124, 64, 64)             496\n",
      "       Activation-15                           (64, 124, 64, 64)               0\n",
      "           Conv2D-16                            (64, 20, 64, 64)           22340\n",
      "       DenseBlock-17                           (64, 144, 64, 64)               0\n",
      "          Dropout-18                           (64, 144, 64, 64)               0\n",
      "        BatchNorm-19                           (64, 144, 64, 64)             576\n",
      "       Activation-20                           (64, 144, 64, 64)               0\n",
      "           Conv2D-21                            (64, 56, 64, 64)            8120\n",
      "        AvgPool2D-22                            (64, 56, 32, 32)               0\n",
      "        BatchNorm-23                            (64, 56, 32, 32)             224\n",
      "       Activation-24                            (64, 56, 32, 32)               0\n",
      "           Conv2D-25                            (64, 20, 32, 32)           10100\n",
      "        BatchNorm-26                            (64, 76, 32, 32)             304\n",
      "       Activation-27                            (64, 76, 32, 32)               0\n",
      "           Conv2D-28                            (64, 20, 32, 32)           13700\n",
      "        BatchNorm-29                            (64, 96, 32, 32)             384\n",
      "       Activation-30                            (64, 96, 32, 32)               0\n",
      "           Conv2D-31                            (64, 20, 32, 32)           17300\n",
      "        BatchNorm-32                           (64, 116, 32, 32)             464\n",
      "       Activation-33                           (64, 116, 32, 32)               0\n",
      "           Conv2D-34                            (64, 20, 32, 32)           20900\n",
      "        BatchNorm-35                           (64, 136, 32, 32)             544\n",
      "       Activation-36                           (64, 136, 32, 32)               0\n",
      "           Conv2D-37                            (64, 20, 32, 32)           24500\n",
      "        BatchNorm-38                           (64, 156, 32, 32)             624\n",
      "       Activation-39                           (64, 156, 32, 32)               0\n",
      "           Conv2D-40                            (64, 20, 32, 32)           28100\n",
      "        BatchNorm-41                           (64, 176, 32, 32)             704\n",
      "       Activation-42                           (64, 176, 32, 32)               0\n",
      "           Conv2D-43                            (64, 20, 32, 32)           31700\n",
      "        BatchNorm-44                           (64, 196, 32, 32)             784\n",
      "       Activation-45                           (64, 196, 32, 32)               0\n",
      "           Conv2D-46                            (64, 20, 32, 32)           35300\n",
      "       DenseBlock-47                           (64, 216, 32, 32)               0\n",
      "          Dropout-48                           (64, 216, 32, 32)               0\n",
      "        BatchNorm-49                           (64, 216, 32, 32)             864\n",
      "       Activation-50                           (64, 216, 32, 32)               0\n",
      "           Conv2D-51                           (64, 108, 32, 32)           23436\n",
      "        AvgPool2D-52                           (64, 108, 16, 16)               0\n",
      "        BatchNorm-53                           (64, 108, 16, 16)             432\n",
      "       Activation-54                           (64, 108, 16, 16)               0\n",
      "           Conv2D-55                            (64, 20, 16, 16)           19460\n",
      "        BatchNorm-56                           (64, 128, 16, 16)             512\n",
      "       Activation-57                           (64, 128, 16, 16)               0\n",
      "           Conv2D-58                            (64, 20, 16, 16)           23060\n",
      "        BatchNorm-59                           (64, 148, 16, 16)             592\n",
      "       Activation-60                           (64, 148, 16, 16)               0\n",
      "           Conv2D-61                            (64, 20, 16, 16)           26660\n",
      "        BatchNorm-62                           (64, 168, 16, 16)             672\n",
      "       Activation-63                           (64, 168, 16, 16)               0\n",
      "           Conv2D-64                            (64, 20, 16, 16)           30260\n",
      "        BatchNorm-65                           (64, 188, 16, 16)             752\n",
      "       Activation-66                           (64, 188, 16, 16)               0\n",
      "           Conv2D-67                            (64, 20, 16, 16)           33860\n",
      "        BatchNorm-68                           (64, 208, 16, 16)             832\n",
      "       Activation-69                           (64, 208, 16, 16)               0\n",
      "           Conv2D-70                            (64, 20, 16, 16)           37460\n",
      "        BatchNorm-71                           (64, 228, 16, 16)             912\n",
      "       Activation-72                           (64, 228, 16, 16)               0\n",
      "           Conv2D-73                            (64, 20, 16, 16)           41060\n",
      "        BatchNorm-74                           (64, 248, 16, 16)             992\n",
      "       Activation-75                           (64, 248, 16, 16)               0\n",
      "           Conv2D-76                            (64, 20, 16, 16)           44660\n",
      "        BatchNorm-77                           (64, 268, 16, 16)            1072\n",
      "       Activation-78                           (64, 268, 16, 16)               0\n",
      "           Conv2D-79                            (64, 20, 16, 16)           48260\n",
      "        BatchNorm-80                           (64, 288, 16, 16)            1152\n",
      "       Activation-81                           (64, 288, 16, 16)               0\n",
      "           Conv2D-82                            (64, 20, 16, 16)           51860\n",
      "        BatchNorm-83                           (64, 308, 16, 16)            1232\n",
      "       Activation-84                           (64, 308, 16, 16)               0\n",
      "           Conv2D-85                            (64, 20, 16, 16)           55460\n",
      "        BatchNorm-86                           (64, 328, 16, 16)            1312\n",
      "       Activation-87                           (64, 328, 16, 16)               0\n",
      "           Conv2D-88                            (64, 20, 16, 16)           59060\n",
      "        BatchNorm-89                           (64, 348, 16, 16)            1392\n",
      "       Activation-90                           (64, 348, 16, 16)               0\n",
      "           Conv2D-91                            (64, 20, 16, 16)           62660\n",
      "        BatchNorm-92                           (64, 368, 16, 16)            1472\n",
      "       Activation-93                           (64, 368, 16, 16)               0\n",
      "           Conv2D-94                            (64, 20, 16, 16)           66260\n",
      "        BatchNorm-95                           (64, 388, 16, 16)            1552\n",
      "       Activation-96                           (64, 388, 16, 16)               0\n",
      "           Conv2D-97                            (64, 20, 16, 16)           69860\n",
      "        BatchNorm-98                           (64, 408, 16, 16)            1632\n",
      "       Activation-99                           (64, 408, 16, 16)               0\n",
      "          Conv2D-100                            (64, 20, 16, 16)           73460\n",
      "      DenseBlock-101                           (64, 428, 16, 16)               0\n",
      "         Dropout-102                           (64, 428, 16, 16)               0\n",
      "       BatchNorm-103                           (64, 428, 16, 16)            1712\n",
      "      Activation-104                           (64, 428, 16, 16)               0\n",
      "          Conv2D-105                           (64, 214, 16, 16)           91806\n",
      "       AvgPool2D-106                             (64, 214, 8, 8)               0\n",
      "       BatchNorm-107                             (64, 214, 8, 8)             856\n",
      "      Activation-108                             (64, 214, 8, 8)               0\n",
      "          Conv2D-109                              (64, 20, 8, 8)           38540\n",
      "       BatchNorm-110                             (64, 234, 8, 8)             936\n",
      "      Activation-111                             (64, 234, 8, 8)               0\n",
      "          Conv2D-112                              (64, 20, 8, 8)           42140\n",
      "       BatchNorm-113                             (64, 254, 8, 8)            1016\n",
      "      Activation-114                             (64, 254, 8, 8)               0\n",
      "          Conv2D-115                              (64, 20, 8, 8)           45740\n",
      "       BatchNorm-116                             (64, 274, 8, 8)            1096\n",
      "      Activation-117                             (64, 274, 8, 8)               0\n",
      "          Conv2D-118                              (64, 20, 8, 8)           49340\n",
      "       BatchNorm-119                             (64, 294, 8, 8)            1176\n",
      "      Activation-120                             (64, 294, 8, 8)               0\n",
      "          Conv2D-121                              (64, 20, 8, 8)           52940\n",
      "       BatchNorm-122                             (64, 314, 8, 8)            1256\n",
      "      Activation-123                             (64, 314, 8, 8)               0\n",
      "          Conv2D-124                              (64, 20, 8, 8)           56540\n",
      "       BatchNorm-125                             (64, 334, 8, 8)            1336\n",
      "      Activation-126                             (64, 334, 8, 8)               0\n",
      "          Conv2D-127                              (64, 20, 8, 8)           60140\n",
      "       BatchNorm-128                             (64, 354, 8, 8)            1416\n",
      "      Activation-129                             (64, 354, 8, 8)               0\n",
      "          Conv2D-130                              (64, 20, 8, 8)           63740\n",
      "       BatchNorm-131                             (64, 374, 8, 8)            1496\n",
      "      Activation-132                             (64, 374, 8, 8)               0\n",
      "          Conv2D-133                              (64, 20, 8, 8)           67340\n",
      "       BatchNorm-134                             (64, 394, 8, 8)            1576\n",
      "      Activation-135                             (64, 394, 8, 8)               0\n",
      "          Conv2D-136                              (64, 20, 8, 8)           70940\n",
      "       BatchNorm-137                             (64, 414, 8, 8)            1656\n",
      "      Activation-138                             (64, 414, 8, 8)               0\n",
      "          Conv2D-139                              (64, 20, 8, 8)           74540\n",
      "       BatchNorm-140                             (64, 434, 8, 8)            1736\n",
      "      Activation-141                             (64, 434, 8, 8)               0\n",
      "          Conv2D-142                              (64, 20, 8, 8)           78140\n",
      "       BatchNorm-143                             (64, 454, 8, 8)            1816\n",
      "      Activation-144                             (64, 454, 8, 8)               0\n",
      "          Conv2D-145                              (64, 20, 8, 8)           81740\n",
      "       BatchNorm-146                             (64, 474, 8, 8)            1896\n",
      "      Activation-147                             (64, 474, 8, 8)               0\n",
      "          Conv2D-148                              (64, 20, 8, 8)           85340\n",
      "       BatchNorm-149                             (64, 494, 8, 8)            1976\n",
      "      Activation-150                             (64, 494, 8, 8)               0\n",
      "          Conv2D-151                              (64, 20, 8, 8)           88940\n",
      "       BatchNorm-152                             (64, 514, 8, 8)            2056\n",
      "      Activation-153                             (64, 514, 8, 8)               0\n",
      "          Conv2D-154                              (64, 20, 8, 8)           92540\n",
      "       BatchNorm-155                             (64, 534, 8, 8)            2136\n",
      "      Activation-156                             (64, 534, 8, 8)               0\n",
      "          Conv2D-157                              (64, 20, 8, 8)           96140\n",
      "       BatchNorm-158                             (64, 554, 8, 8)            2216\n",
      "      Activation-159                             (64, 554, 8, 8)               0\n",
      "          Conv2D-160                              (64, 20, 8, 8)           99740\n",
      "       BatchNorm-161                             (64, 574, 8, 8)            2296\n",
      "      Activation-162                             (64, 574, 8, 8)               0\n",
      "          Conv2D-163                              (64, 20, 8, 8)          103340\n",
      "       BatchNorm-164                             (64, 594, 8, 8)            2376\n",
      "      Activation-165                             (64, 594, 8, 8)               0\n",
      "          Conv2D-166                              (64, 20, 8, 8)          106940\n",
      "       BatchNorm-167                             (64, 614, 8, 8)            2456\n",
      "      Activation-168                             (64, 614, 8, 8)               0\n",
      "          Conv2D-169                              (64, 20, 8, 8)          110540\n",
      "       BatchNorm-170                             (64, 634, 8, 8)            2536\n",
      "      Activation-171                             (64, 634, 8, 8)               0\n",
      "          Conv2D-172                              (64, 20, 8, 8)          114140\n",
      "       BatchNorm-173                             (64, 654, 8, 8)            2616\n",
      "      Activation-174                             (64, 654, 8, 8)               0\n",
      "          Conv2D-175                              (64, 20, 8, 8)          117740\n",
      "       BatchNorm-176                             (64, 674, 8, 8)            2696\n",
      "      Activation-177                             (64, 674, 8, 8)               0\n",
      "          Conv2D-178                              (64, 20, 8, 8)          121340\n",
      "       BatchNorm-179                             (64, 694, 8, 8)            2776\n",
      "      Activation-180                             (64, 694, 8, 8)               0\n",
      "          Conv2D-181                              (64, 20, 8, 8)          124940\n",
      "       BatchNorm-182                             (64, 714, 8, 8)            2856\n",
      "      Activation-183                             (64, 714, 8, 8)               0\n",
      "          Conv2D-184                              (64, 20, 8, 8)          128540\n",
      "       BatchNorm-185                             (64, 734, 8, 8)            2936\n",
      "      Activation-186                             (64, 734, 8, 8)               0\n",
      "          Conv2D-187                              (64, 20, 8, 8)          132140\n",
      "       BatchNorm-188                             (64, 754, 8, 8)            3016\n",
      "      Activation-189                             (64, 754, 8, 8)               0\n",
      "          Conv2D-190                              (64, 20, 8, 8)          135740\n",
      "       BatchNorm-191                             (64, 774, 8, 8)            3096\n",
      "      Activation-192                             (64, 774, 8, 8)               0\n",
      "          Conv2D-193                              (64, 20, 8, 8)          139340\n",
      "       BatchNorm-194                             (64, 794, 8, 8)            3176\n",
      "      Activation-195                             (64, 794, 8, 8)               0\n",
      "          Conv2D-196                              (64, 20, 8, 8)          142940\n",
      "       BatchNorm-197                             (64, 814, 8, 8)            3256\n",
      "      Activation-198                             (64, 814, 8, 8)               0\n",
      "          Conv2D-199                              (64, 20, 8, 8)          146540\n",
      "       BatchNorm-200                             (64, 834, 8, 8)            3336\n",
      "      Activation-201                             (64, 834, 8, 8)               0\n",
      "          Conv2D-202                              (64, 20, 8, 8)          150140\n",
      "      DenseBlock-203                             (64, 854, 8, 8)               0\n",
      "         Dropout-204                             (64, 854, 8, 8)               0\n",
      "       BatchNorm-205                             (64, 854, 8, 8)            3416\n",
      "      Activation-206                             (64, 854, 8, 8)               0\n",
      " GlobalAvgPool2D-207                             (64, 854, 1, 1)               0\n",
      "           Dense-208                                     (64, 6)            5130\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 4245508\n",
      "   Trainable params: 4197536\n",
      "   Non-trainable params: 47972\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 4245508\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for X,_ in train_iter:\n",
    "    net_DENSENET.summary(X.as_in_context(ctx))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = mx.optimizer.Adam(learning_rate=1e-4, beta1=0.9, beta2=0.9)\n",
    "trainer = gluon.Trainer(net_DENSENET.collect_params(), optimizer=adam_optimizer)\n",
    "train(net_DENSENET, train_iter, valid_iter, batch_size, trainer, ctx, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Accuracy for Testing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model is 75.0000 % .\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of model is %.4f %% .'%(100*evaluate_accuracy(test_iter, net_DENSENET, ctx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Accuracy for Extra Validation Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model is 33.9286 % .\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of model is %.4f %% .'%(100*evaluate_accuracy(extra_validation_iter, net_DENSENET, ctx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Saving and Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_DENSENET.save_parameters('C:/Users/lab/Jupyter_Notebook/Guan-Ming/Midterm_Project/NN_params/train_test_mixed/DENSENET_DROPOUT_train_data_transposed_2.params')\n",
    "#===============================================================\n",
    "# net_DENSENET = nn.Sequential()\n",
    "# net_DENSENET.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3), \n",
    "#         nn.BatchNorm(),\n",
    "#         nn.Activation('relu'), \n",
    "#         nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "\n",
    "# num_channels, growth_rate = 32, 16\n",
    "# num_convs_in_dense_blocks = [4, 8, 16, 32, 64]\n",
    "\n",
    "# for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "#     net_DENSENET.add(DenseBlock(num_convs, growth_rate))\n",
    "#     net_DENSENET.add(nn.Dropout(0.15))\n",
    "# #     # This is the number of output channels in the previous dense block \n",
    "#     num_channels += num_convs * growth_rate\n",
    "#     # A transition layer that haves the number of channels is added between\n",
    "#     # the dense blocks \n",
    "#     if i != len(num_convs_in_dense_blocks) - 1: \n",
    "#         num_channels //= 2\n",
    "#         net_DENSENET.add(transition_block(num_channels))\n",
    "                    \n",
    "# net_DENSENET.add(nn.BatchNorm(), \n",
    "#         nn.Activation('relu'), \n",
    "#         nn.GlobalAvgPool2D(),\n",
    "#         nn.Dense(6))\n",
    "#==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = np.zeros(total_test_data)\n",
    "count = 0\n",
    "for X, _ in test_iter:\n",
    "    y_pred_test[count:min(count+batch_size, total_test_data)] = net_DENSENET(X.as_in_context(ctx)).asnumpy().argmax(axis=1)\n",
    "    count += batch_size\n",
    "wrong_index_test = np.argwhere(y_pred_test!=test_data_y.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46  4  0  0  0  0]\n",
      " [ 5 44  0  0  0  1]\n",
      " [ 1  9 34  6  0  0]\n",
      " [ 1  7  0 38  3  1]\n",
      " [ 1  3  3  4 26 13]\n",
      " [ 0  3  1  6  3 37]]\n",
      "\n",
      "\n",
      "Accuracy of 0 is 92.00 %.\n",
      "Accuracy of 1 is 88.00 %.\n",
      "Accuracy of 2 is 68.00 %.\n",
      "Accuracy of 3 is 76.00 %.\n",
      "Accuracy of 4 is 52.00 %.\n",
      "Accuracy of 5 is 74.00 %.\n"
     ]
    }
   ],
   "source": [
    "result = confusion_matrix(test_data_y.asnumpy(), y_pred_test)\n",
    "print(result)\n",
    "print('\\n')\n",
    "for i in range(6):\n",
    "    print(\"Accuracy of %d is %.2f %%.\" %(i, 100*result[i][i]/result[i].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 75 wrong predcition. Slide to see the corresponding image.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dcacb9571d4f16a7d3c665fcb7ca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index_selected', max=74), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PLOT_WRONG_PREDICTION(index_selected):\n",
    "    print(index_selected)\n",
    "    plt.figure(figsize=(3,3), dpi=120)\n",
    "    plt.imshow(test_data_x[wrong_index_test[index_selected]][0].asnumpy().astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"NN Recognizes as %d .\\nReal Answer is %d .\" %(y_pred_test[wrong_index_test[index_selected]], test_data_y[wrong_index_test[index_selected]].asscalar()) )\n",
    "    \n",
    "print('Total %d wrong predcition. Slide to see the corresponding image.' %len(wrong_index_test))\n",
    "interact(PLOT_WRONG_PREDICTION, index_selected=IntSlider(value=0, min=0, max=len(wrong_index_test)-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix of Extra Validation Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_extra_validation = np.zeros(total_extra_validation_data)\n",
    "count = 0\n",
    "for X, _ in extra_validation_iter:\n",
    "    y_pred_extra_validation[count:count+batch_size] = net_DENSENET(X.as_in_context(ctx))[:total_extra_validation_data].asnumpy().argmax(axis=1)\n",
    "    count += batch_size\n",
    "wrong_index_extra_validation = np.argwhere(y_pred_extra_validation!=extra_validation_data_y.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0]\n",
      " [1 4 0 1 0 0]\n",
      " [0 0 3 2 0 1]\n",
      " [1 2 6 9 2 7]\n",
      " [0 0 2 2 0 1]\n",
      " [0 1 2 6 0 1]]\n",
      "\n",
      "\n",
      "Accuracy of 0 is 100.00 %.\n",
      "Accuracy of 1 is 66.67 %.\n",
      "Accuracy of 2 is 50.00 %.\n",
      "Accuracy of 3 is 33.33 %.\n",
      "Accuracy of 4 is 0.00 %.\n",
      "Accuracy of 5 is 10.00 %.\n"
     ]
    }
   ],
   "source": [
    "result = confusion_matrix(extra_validation_data_y.asnumpy(), y_pred_extra_validation)\n",
    "print(result)\n",
    "print('\\n')\n",
    "for i in range(6):\n",
    "    print(\"Accuracy of %d is %.2f %%.\" %(i, 100*result[i][i]/result[i].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 37 wrong predcition. Slide to see the corresponding image.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486ed55d80404d4cb42eb425819c7c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index_selected', max=36), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PLOT_WRONG_PREDICTION(index_selected):\n",
    "    print(index_selected)\n",
    "    plt.figure(figsize=(3,3), dpi=120)\n",
    "    plt.imshow(extra_validation_data_x[wrong_index_extra_validation[index_selected]][0].asnumpy().astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"NN Recognizes as %d .\\nReal Answer is %d .\" %(y_pred_extra_validation[wrong_index_extra_validation[index_selected]], extra_validation_data_y[wrong_index_extra_validation[index_selected]].asscalar()) )\n",
    "    \n",
    "print('Total %d wrong predcition. Slide to see the corresponding image.' %len(wrong_index_extra_validation))\n",
    "interact(PLOT_WRONG_PREDICTION, index_selected=IntSlider(value=0, min=0, max=len(wrong_index_extra_validation)-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
